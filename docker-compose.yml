services:
  postgres:
    image: postgres:14-alpine
    restart: always
    # comment out if you want to externally connect DB
    # ports:
    #   - 5432:5432
    volumes:
      - ./postgres-data:/var/lib/postgresql/data
    environment:
      - PGDATA=/var/lib/postgresql/data/pgdata
      - POSTGRES_USER=skyvern
      - POSTGRES_PASSWORD=skyvern
      - POSTGRES_DB=skyvern
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U skyvern"]
      interval: 5s
      timeout: 5s
      retries: 5

  skyvern:
    image: public.ecr.aws/skyvern/skyvern:latest
    restart: on-failure
    # comment out if you want to externally call skyvern API
    ports:
      - 8000:8000
      - 9222:9222 # for cdp browser forwarding
    volumes:
      - ./artifacts:/data/artifacts
      - ./videos:/data/videos
      - ./har:/data/har
      - ./log:/data/log
      - ./.streamlit:/app/.streamlit
      # Uncomment the following two lines if you want to connect to any local changes
      # - ./skyvern:/app/skyvern
      # - ./alembic:/app/alembic
    environment:
      - DATABASE_STRING=postgresql+psycopg://skyvern:skyvern@postgres:5432/skyvern
      - BROWSER_TYPE=chromium-headful
      - ENABLE_CODE_BLOCK=true
      # - BROWSER_TYPE=cdp-connect
      # Use this command to start Chrome with remote debugging:
      # "C:\Program Files\Google\Chrome\Application\chrome.exe" --remote-debugging-port=9222 --user-data-dir="C:\chrome-cdp-profile" --no-first-run --no-default-browser-check
      # /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222 --user-data-dir="/Users/yourusername/chrome-cdp-profile" --no-first-run --no-default-browser-check
      # - BROWSER_REMOTE_DEBUGGING_URL=http://host.docker.internal:9222/

      # =========================
      #       LLM Settings
      # =========================
      
      # 다른 LLM 제공자 비활성화
      - ENABLE_OPENAI=false
      - ENABLE_GEMINI=false
      # - GEMINI_API_KEY=YOUR_GEMINI_KEY # ENABLE_GEMINI=false 이므로 주석처리 해도 무방
      # - LLM_KEY=GEMINI_2.5_PRO_PREVIEW_03_25 # LLM_KEY는 아래에서 OpenAI Compatible 용으로 덮어씀

      # - ENABLE_ANTHROPIC=false # 필요시 추가
      # - ANTHROPIC_API_KEY=<your_anthropic_key>

      # - ENABLE_AZURE=false # 필요시 추가
      # (Azure 관련 변수들 생략)

      # - ENABLE_BEDROCK=false # 필요시 추가
      # (Bedrock 관련 변수들 생략)

      - ENABLE_OLLAMA=false # Ollama 직접 사용은 비활성화
      # - OLLAMA_MODEL=mistral-small-3.1-24b-instruct-2503
      # - OLLAMA_SERVER_URL=http://host.docker.internal:1236/v1

      # OpenAI Compatible (LM Studio) 설정
      - ENABLE_OPENAI_COMPATIBLE=true
      - OPENAI_COMPATIBLE_MODEL_KEY=LMSTUDIO_MISTRAL # Skyvern 내부 설정 키
      - OPENAI_COMPATIBLE_API_BASE=http://host.docker.internal:1236/v1
      - OPENAI_COMPATIBLE_API_KEY="sk-dummykey" # LM Studio는 키 불필요, 형식상 제공
      - OPENAI_COMPATIBLE_MODEL_NAME=mistral-small-3.1-24b-instruct-2503 # 실제 모델 ID
      
      # (선택 사항) OpenAI Compatible 추가 설정들
      # - OPENAI_COMPATIBLE_API_VERSION=
      # - OPENAI_COMPATIBLE_MAX_TOKENS=
      # - OPENAI_COMPATIBLE_TEMPERATURE=
      - OPENAI_COMPATIBLE_SUPPORTS_VISION=true # 대부분의 LM Studio 모델은 비전 미지원
      # - OPENAI_COMPATIBLE_ADD_ASSISTANT_PREFIX=false # 필요에 따라 true/false
      # - OPENAI_COMPATIBLE_REASONING_EFFORT= 

      # Skyvern이 사용할 최종 LLM_KEY
      - LLM_KEY=LMSTUDIO_MISTRAL # OPENAI_COMPATIBLE_MODEL_KEY와 동일하게 설정

      # Open Router Support:
      # - ENABLE_OPENROUTER=true
      # - LLM_KEY=OPENROUTER
      # - OPENROUTER_API_KEY=<your_openrouter_api_key>
      # - OPENROUTER_MODEL=mistralai/mistral-small-3.1-24b-instruct
      # Groq Support:
      # - ENABLE_GROQ=true
      # - LLM_KEY=GROQ
      # - GROQ_API_KEY=<your_groq_api_key>
      # - GROQ_MODEL=llama-3.1-8b-instant

      # Maximum tokens to use: (only set for OpenRouter aand Ollama)
      # - LLM_CONFIG_MAX_TOKENS=128000

      # Bitwarden Settings
      # If you are looking to integrate Skyvern with a password manager (eg Bitwarden), you can use the following environment variables.
      # - BITWARDEN_SERVER=http://localhost  # OPTIONAL IF YOU ARE SELF HOSTING BITWARDEN
      # - BITWARDEN_SERVER_PORT=8002 # OPTIONAL IF YOU ARE SELF HOSTING BITWARDEN
      # - BITWARDEN_CLIENT_ID=FILL_ME_IN_PLEASE
      # - BITWARDEN_CLIENT_SECRET=FILL_ME_IN_PLEASE
      # - BITWARDEN_MASTER_PASSWORD=FILL_ME_IN_PLEASE
      
      # LM Studio를 위한 OpenAI Compatible 설정
      # - ENABLE_OPENAI=true
      # - LLM_KEY는 LiteLLM이 모델을 식별하고 OpenAI 클라이언트를 사용하도록 하는 키입니다.
      # - OPENAI_MODEL_NAME에 실제 모델 ID를 지정하므로, LLM_KEY는 해당 API_BASE에서 사용될 대표 모델명 정도로 이해할 수 있습니다.
      # - Skyvern 내부적으로 특정 LLM_KEY 문자열 (예: "OPENAI_GPT_DEFAULT")에 OPENAI_API_BASE를 연결할 수도 있습니다.
      # - 가장 안전한 방법은 실제 OpenAI 모델 이름 중 하나를 사용하는 것이지만, 호환 엔드포인트에서는 중요하지 않을 수 있습니다.
      # - 여기서는 "custom-openai-model" 같은 일반적인 이름으로 설정하거나, OPENAI_MODEL_NAME으로 대체될 것을 기대하고 간단히 설정합니다.
      # - LLM_KEY=mistral-small-3.1-24b-instruct-2503 # 또는 "custom-model" 등, OPENAI_MODEL_NAME에 의해 덮어쓰여질 수 있음
      # - OPENAI_API_KEY="sk-dummykey" # LM Studio는 API 키가 필요 없지만, 형식상 임의의 값을 제공
      # - OPENAI_API_BASE=http://host.docker.internal:1236/v1 # LM Studio의 OpenAI 호환 기본 URL (포트: 1234 가정)
      # - OPENAI_MODEL_NAME은 LiteLLM을 통해 실제 API 호출 시 model 파라미터로 전달될 모델 ID
      # - OPENAI_MODEL_NAME=mistral-small-3.1-24b-instruct-2503 # LM Studio에 로드된 모델 ID (가정)

    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "test", "-f", "/app/.streamlit/secrets.toml"]
      interval: 5s
      timeout: 5s
      retries: 5

  skyvern-ui:
    image: public.ecr.aws/skyvern/skyvern-ui:latest
    restart: on-failure
    ports:
      - 8080:8080
      - 9090:9090
    volumes:
      - ./artifacts:/data/artifacts
      - ./videos:/data/videos
      - ./har:/data/har
      - ./.streamlit:/app/.streamlit
    environment:
      - VITE_ENABLE_CODE_BLOCK=true
    # if you want to run skyvern on a remote server,
    # you need to change the host in VITE_WSS_BASE_URL and VITE_API_BASE_URL to match your server ip
    # If you're self-hosting this behind a dns, you'll want to set:
    #   A route for the API: api.yourdomain.com -> localhost:8000
    #   A route for the UI: yourdomain.com -> localhost:8080
    #   A route for the artifact API: artifact.yourdomain.com -> localhost:9090 (maybe not needed)
      - VITE_WSS_BASE_URL=ws://localhost:8000/api/v1
    #   - VITE_ARTIFACT_API_BASE_URL=http://localhost:9090
    #   - VITE_API_BASE_URL=http://localhost:8000/api/v1
    #   - VITE_SKYVERN_API_KEY=<get this from "settings" in the Skyvern UI>
    depends_on:
      skyvern:
        condition: service_healthy
